## 基本概念

### 概率与统计

概率

* 研究事件的不确定性，在给定数据生成过程中观察、研究数据的性质，强调公理体系、推理
* 研究随机变量、分布、大数定律、概率不等式等

统计

* **收集和分析数据**，根据观察的数据反思其数据生成过程，强调归纳
* 研究推断、参数估计、假设检验、模型、算法等（与机器学习相关）

### 随机现象的二重性

二重性：偶然性与必然性

* 偶然性：对随机现象做一次观察，观测结果不可预知
* 必然性：对随机现象做大量观察，观察结果具有一定的规律性，即统计规律性

### 随机试验

随机试验：具备以下==三个特点==的实验：

* 可重复：可在相同条件下重复进行
* 多结果：结果不止一个，**而且所有可能的结果事先已知**
* 不确定：试验前无法预测/确定是哪一种结果

### 样本空间，样本点，随机事件

* 样本点：试验的每一种可能结果，记为$\omega$
* 样本空间：试验中所有可能结果组成的集合，是所有样本点的集合，记为$\Omega$
* 随机事件：样本空间的子集，是由单个或多个样本点的集合
  * “随机事件A发生“当且仅当试验的结果是子集A的元素
  * 例如抛两枚骰子，样本点为二维空间中的点，A为点数相同
  * 基本事件：单个样本点构成的集合
  * 必然事件：全集
  * 不可能事件：空集
  * 事件间的关系：
    * ~~包含（A发生必然B发生，则B包含A）~~
    * ~~相等（互相包含）~~
    * 互斥（互不相容），**不能同时发生，交为空集**
    * 对立（逆）事件，事件A不发生，记为 $\bar A$ ，**交集为空且并集为全集**

==$A-B$ 的多种表现形式：$A-B=A-AB=A\bar B=(A\cup B)-B$==

## 概率公理化

### 频率与概率

* 概率用于度量事件发生的可能性
* 频率在一定程度上反映了事件发生可能性
* 概率恒定，频率在试验中有随机性
* 若试验次数够多，两者非常接近
* 概率可以通过频率来测量，频率是概率的一个近似
* 频率的稳定性：大量重复试验中，通常在常数 $p$ 附近摆动，随次数增大摆幅越来越小

### ==概率的公理化==

在随机试验的样本空间 $\Omega$ ，对每一个事件 A 赋予一个实数，记为 $P(A)$ ，若满足下列条件，称 $P(A)$ 为 A 发生的概率

* 非负性：$P(A)\geqslant 0$
* 规范性：$P(\Omega)=1$
* ==可列可加性==：若可列个事件 $A_1,A_2\cdots$ 两两互不相容，则 $P(\bigcup_{i=1}^\infty A_i)=\sum_{i=1}^\infty P(A_i)$

### ==容斥原理==

$P(A\cup B)=P(A)+P(B)-P(AB)$

多事件：

$\displaystyle P(\bigcup_{i=1}^n A_i)=\sum_{i=1}^n P(A_i)-\sum_{i<j}^n P(A_iA_j)+\sum_{i<j<k}^n P(A_iA_jA_k)+\cdots+(-1)^{n-1}P(A_1\cdots A_n)$

简化版：

$\displaystyle P(\bigcup_{i=1}^n A_i)=\sum_{r=1}^n(-1)^{r+1}\sum_{i_1<\cdots i_r}P(A_{i_1}\cdots A_{i_r})$

证明：数学归纳法

### Union-bound

$\displaystyle P(\bigcup_{i=1}^n A_i)\leqslant \sum_{i=1}^n P(A_i)$

## 古典和几何概型

### 古典概型

#### 基本计数原理

加法原理，乘法原理

#### 排列

$r=n$ 时称为全排列

简写：$\displaystyle(n)_k=\frac{n!}{(n-k)!}$

比如：$\displaystyle(365)_k=\frac{365!}{(365-k)!}$

#### 组合

注意组合的简写：

$\displaystyle \begin{pmatrix}n\\r\end{pmatrix}=C_n^r=\frac{n!}{r!(n-r)!}$

称为组合数与二项系数

#### 超几何分布

设一批 $𝑁$ 件产品中有 $𝑀$ 件次品, 现从 $𝑁$ 件产品中不放回地任选 $n$ 件, 求其中恰有 $k$ 件次品的事件 $A$ 的概率 $P(A)$

$\displaystyle P(A)=\frac{\begin{pmatrix}M\\k\end{pmatrix}\begin{pmatrix}N-M\\n-k\end{pmatrix}}{\begin{pmatrix}N\\n\end{pmatrix}}$

### ==几何概型==

* 样本空间无限可测（无限不可列），可以用几何图形表示，其相应的几何测度是一个非零有限的实数
* 每个基本事件发生的可能性大小相等，从而每个事件发生的概率与该事件的几何测度相关，与具体位置无关

#### 严格定义

在一个测度有限的区域 $\Omega$  内等可能性投点，落入其中的任意子区域 $A$ 的可能性和 $A$ 的测度成正比，与 $A$ 的位置与形状无关，这样的概率模型称为几何概型

事件 $A$ 发生的概率为 $\displaystyle P(A)=\frac{A的测度}{\Omega 的测度}=\frac{\mu(A)}{\mu(\Omega)}$

## 条件概率与独立性

考虑在一定条件下某一随机事件发生的概率

### ==定义==

设 $A$ 和 $B$ 为同一样本空间下的随机事件，且 $P(A)>0$，则

$\displaystyle P(B|A)=\frac{P(AB)}{P(A)}$

称为 $A$ 发生的条件下事件 $B$ 发生的概率

### ==乘法公式==

对于随机事件 $A,B,\quad P(A)>0$，有

$P(AB)=P(A)P(B|A)=P(B)P(A|B)$

### 全概率公式（Law of total probability）

若随机事件 $A_1,\cdots,A_n$ 满足：

* 两两互不相容
* 完备性：$\Omega=\bigcup_{i=1}^nA_i$

那么记事件 $A_1,\cdots,A_n$ 为样本空间 $\Omega$ 的一个划分

如果事件 $A_1,\cdots,A_n$ 为样本空间 $\Omega$ 的一个划分，对于任意事件 $B$ 有：

$\displaystyle P(B)=\sum_{i=1}^n P(BA_i)=\sum_{i=1}^nP(A_i)P(B|A_i)$

### ==贝叶斯公式（Bayes' law）==

设事件 $A_1,\cdots,A_n$ 为样本空间 $\Omega$ 的一个划分，且事件 $B$ 满足 $P(B)>0$

对任意 $1\leq i\leq n$

$\displaystyle P\left(A_{i} \mid B\right)=\frac{P\left(A_{i} B\right)}{P(B)}=\frac{P\left(A_{i}\right) P\left(B \mid A_{i}\right)}{\sum_{j=1}^{n} P\left(A_{j}\right) P\left(B \mid A_{j}\right)}$

贝叶斯公式的一种直觉解释: 将事件 $B$ 看作结果, 将 $A_{1}, A_{2}, \cdots, A_{n}$ 看作产生结果的若干种原因, 如果

1. 每一种原因发生的概率 $P\left(A_{i}\right)$ 已知;
2. 每一种原因 $A_{i}$ 对结果 $B$ 的影响已知, 即概率 $P\left(B \mid A_{i}\right)$ 已知

则可求**事件 $B$ 由第 $i$ 种原因引起的概率** $P\left(A_{i} \mid B\right)$。

==贝叶斯公式中每项都有特定的名称:== 

* ==$\operatorname{Pr}\left(A_{i}\right)$ 被称为事件 $A_{i}$ 的**先验 (prior) 概率**，之所有称为“先验”是因为不考虑事件 $B$ 的任何因素。==
* ==$P(B)=\sum_{j=1}^{n} P\left(A_{j}\right) P\left(B \mid A_{j}\right)$ 被称为**证据 (evidence) 概率**==
* ==$\operatorname{Pr}\left(A_{i} \mid B\right)$ 被称为事件 $A_{i}$ 在事件 $B$ (证据) 发生的情况下的**后验 (posterior) 概率**==
* ==$P\left(B \mid A_{i}\right)$ 被称为**似然度 (likelihood)**==

因此贝叶斯公式可以进一步写为

==后验概率 $=\displaystyle\frac{\text { 先验概率 } \times \text { 似然度 }}{\text { 证据概率 }}=$ 常量 $\times$ 似然度==

由此可知==后验概率与似然度成正比==

对贝叶斯公式, 当 $n=2$ 时有：

推论 $2.1$ 对事件 $A$ 和 $B$ 且满足 $P(B)>0$, 有

$\displaystyle P(A \mid B)=\frac{P(A B)}{P(B)}=\frac{P(A) P(B \mid A)}{P(A) P(B \mid A)+P(\bar{A}) P(B \mid \bar{A})}$

### 两事件独立性

若事件 $A,B$ 满足 $P(AB)=P(A)P(B)$ ，称事件 $A,B$ 相互独立

此时事件 $A$ 的发生对 $B$ 没有影响，反正亦然

任何事件与必然事件或不可能事件相互独立

### 互斥与独立

独立性与概率相关，反映事件的概率属性

互斥（互不相容）是事件的运算关系，与概率无关

如果两事件 $A,B$ 满足 $P(A)P(B)>0$ ，则有：

* 互质则不独立
* 独立则不互斥

### ==多事件独立性==

若事件 $A_1,A_2,\cdots A_n$ 中任意 $k$ 个事件独立，即对任意 $k\in [n]$ 有：

$P(A_{i_1}\cdots A_{i_k})=P(A_{i_1})\cdots P(A_{i_k})$

其中 $1\leq i_1\leq i_2\leq \cdots \leq n$

则称 $A_1,A_2,\cdots A_n$ 相互独立

==与两两独立区分：两两独立不一定多事件独立==

### 小概率原理

若事件 $A$ 在一次试验中发生的概率非常小，但经过多次独立地重复试验，事件 $A$ 的发生是必然的，称之为**小概率原理**

## 离散型随机变量

注意：==离散型随机变量 $+$ 连续型随机变量 $\ne$ 所有随机变量==

### 定义

离散型随机变量 $X$ 的取值是**有限或可列**的（否则称为非离散型随机变量）

不妨假设其取值为 $x_1,x_2,\cdots,x_k,\cdots$ ，事件 $X=x_k$ 的概率记为

$p_k=P(X=x_k)\quad k=1,2,\cdots$

称为随机变量 $X$ 的分布列

分布列包含离散型随机变量的取值和概率，完全刻画了其概率属性

### 性质

**非负性**：$p_k\geq0$（严格地，$p_k>0$，因为离散型考虑 $0$ 是没有意义的）

**完备性**：$\sum_{k} p_k=1$

### 期望与方差

若级数 $\displaystyle\sum_{k=1}^\infty p_kx_k$ **绝对收敛**，称其为随机变量 $X$ 的期望，又称均值，记为 $E(X)$

$\displaystyle E(X)=\sum_{k=1}^\infty p_kx_k$

级数的绝对收敛保证级数和不随次序的改变而改变

期望是一阶统计量

==$Var(X)=E[(X-E(X))^2]=E(X^2)-[E(X)]^2$==

* ==对于随机变量 $X$ 和常数 $a\in R$，有：$Var(X)=E[(X-E(X))^2]\leq E[(X-a)^2]$==
* ==对 $X\in[a,b]$，有 $Var(X)\leq (b-E(X))(E(X)-a)\leq(b-a)^2/4$==

#### 数乘和加法

$E(aX+b)=aE(X)+b$

$Var(aX+b)=a^2Var(X)$

#### 与函数的复合

设 $X$ 为离散型随机变量，$g:R\rightarrow R$ 是连续函数，$\sum_{k\geq1}g(x_k)p_k$ 绝对收敛，则有：

$\displaystyle E[g(X)]=\sum_{k=1}^\infty g(x_k)p_k$

即求 $Y=g(X)$ 的期望不需要 $Y$ 的分布列

### Jensen不等式

实际运用中，我们往往不知道随机变量的分布，但是需要对期望进行一定的估计，为此需要引入一些不等式

下面探讨 $g(E(X))$ 与 $E(g(X))$ 的大小关系（ $X\in[a,b],\quad g:[a,b]\to R$ ）

* ==当 $g:[a,b]\to R$ 为**连续凸函数** $g(E(X))\leq E(g(X))$==
* ==当 $g:[a,b]\to R$ 为**连续凹函数** $g(E(X))\geq E(g(X))$==

### 常用离散型分布

| 名称         | 表示         | 分布                                                         | 期望                                             | 方差                                                         |
| ------------ | ------------ | ------------------------------------------------------------ | ------------------------------------------------ | ------------------------------------------------------------ |
| 离散均匀分布 |              | $P\left(X=x_{i}\right)=1 / n$                                | $\displaystyle\frac{1}{n} \sum_{i=1}^{n} x_{i} $ | $\displaystyle\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}-\left(\frac{1}{n} \sum_{i=1}^{n} x_{i}\right)^{2}$ |
| 0/1分布      | $Ber(p)$     | $P(X=1)=p,\quad P(x=0)=1-p$                                  | $p$                                              | $p(1-p)$                                                     |
| 二项分布     | $B(n,p)$     | $n$ 重 Bernoulli 试验中事件 $A$ 发生的次数<br>$\displaystyle P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$ | $np$                                             | $np(1-p)$                                                    |
| ==几何分布== | $G(p)$       | ==事件 $A$ 在 Bernoulli 试验中首次发生时的试验次数==<br>$P(X=k)=(1-p)^{k-1}p$ | ==$\displaystyle\frac1p$==                       | ==$\displaystyle\frac{1-p}{p^2}$==                           |
| 负二项分布   | $NB(r,p)$    | 事件 $A$ 第 $r$ 次成功时发生的试验次数<br>$\displaystyle P(X=n)=\binom{n-1}{r-1}p^{r-1}(1-p)^{n-r}p\quad\quad(n\geq r)$ | ==$\displaystyle\frac{r}{p^2}$==                 | ==$\displaystyle\frac{r(1-p)}{p^2}$==                        |
| ==泊松分布== | $P(\lambda)$ | ==$\displaystyle P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}$==  | ==$\lambda$==                                    | ==$\lambda$==                                                |

仅几何分布具有无记忆性：$P(X>m+n|X>m)=p(X>n)$

==**泊松定理：**==

对任意常数 $\lambda>0$ ， $n$ 为任意正整数，设 $np_n=\lambda$ ，则对任意给定的非负整数 $k$ 有：

$\displaystyle \lim_{n\to \infty}\binom{n}{k}p_n^k(1-p_n)^{n-k}=\frac{\lambda^k}{k!}e^{-\lambda}$

这样就可以用泊松分布近似二项分布

## 连续型随机变量

分布函数：

* 单调性，$x_1<x_2\to F(x_1)\leq F(x_2)$
* 规范性，$F(-\infty)=0,F(+\infty)=1,F(x)\in[0,1]$
* ==右连续性==，$\displaystyle F(x+0)=\lim_{\triangle x\to0}F(x+\triangle x)=F(x)$

概率密度函数：

* 非负性，$f(x)\geq 0$
* 规范性，$\displaystyle \int_{-\infty}^{+\infty} f(t)dt=1$

对于连续型随机变量 $X$ ，其分布函数 $F(x)$ 在实数域上连续

如果 $f(x)$ 在 $x$ 点处连续，则 $F(x)$ 在 $x$ 点处可导，且 $F'(x)=f(x)$

### 期望

$E(X)=\int_{-\infty}^{+\infty}xf(x)dx$

要求绝对收敛

离散型随机变量的期望性质，连续型也有：

* 数乘和加法
* 复合
  * 推论：$\displaystyle E(\sum_{i=1}^n c_i g_i(X))=\sum_{i=1}^n c_i E(g_i(X))$
* $\displaystyle E(g(X))=\int_{-\infty}^{+\infty}g(t)f(t)dt$，f为概率密度函数
* Jensen不等式

==期望的其他计算公式：==

* 对非负随机变量 $X$，$\displaystyle E[X]=\int_0^{+\infty}P(X>t)dt$
* 对非负随机变量 $g(X)$，$\displaystyle E[X]=\int_0^{+\infty}P(g(X)>t)dt$

方差和离散一样

| 名称         | 表示              | 分布                                                         | 期望                             | 方差                                  |
| ------------ | ----------------- | ------------------------------------------------------------ | -------------------------------- | ------------------------------------- |
| 均匀         | $U(a,b)$          | $\displaystyle f(x)=\left\{\begin{matrix}\frac{1}{b-a} &x\in[a,b]\\0&otherwise\end{matrix}\right.$ | $\displaystyle\frac{a+b}{2}$     | ==$\displaystyle\frac{(b-a)^2}{12}$== |
| ==指数==     | $e(\lambda)$      | $\displaystyle f(x)=\left\{\begin{matrix}\lambda e^{-\lambda x}&x>0\\0&otherwise\end{matrix}\right.$ | $\displaystyle\frac{1}{\lambda}$ | $\displaystyle\frac{1}{\lambda^2}$    |
| ==正态分布== | $N(\mu,\sigma^2)$ | $f(x)=\displaystyle\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\mu-x)^2}{2\sigma^2}}$ | $\mu$                            | $\sigma^2$                            |

==正态分布的转换==：

* 若 $X \sim \mathrm{N}\left(\mu, \sigma^{2}\right)$, 则 $Y=(X-\mu) / \sigma \sim N(0,1)$
* 若 $X \sim N(0,1)$, 则 $Y=\sigma X+\mu \sim N\left(\mu, \sigma^{2}\right)$ 

已知连续随机变量 $X$ 的概率密度为 $f_{X}(x)$, 求随机变量 $Y=g(X)$ 的概率密度 $f_{Y}(y) ?$ 

* 求解 $\mathrm{Y}=\mathrm{g}(\mathrm{X})$ 的分布函数 $F_{Y}(y)=P(Y \leq y)=\int_{g(x) \leq y} f_{X}(x) d x$
* 利用分布函数和概率密度关系求解密度函数 $f_{Y}(y)=F_{Y}^{\prime}(y)$

==正态分布：$P\left(X^{\prime} \geqslant \epsilon\right) \leqslant \frac{1}{2} e^{-\epsilon^{2} / 2}$==

## 多维随机变量

### 二维

#### 分布函数

$F(x,y)=P(X\leq x,Y\leq y)$

单调性：分布函数对每个变量单调递增（不一定是严格单调）

规范性：

* $F(+\infty,+\infty)=1$
* $F(-\infty,y)=F(x,-\infty)=F(-\infty,-\infty)=0$
* ==通过规范性，最多可以求三个参数==

右连续性：

关于每个变量右连续。

#### 边缘分布函数

称 $\displaystyle F_X(x)=P(X\leq x)=P(X\leq x,Y\leq +\infty)=\lim_{y\to+\infty}F(x,y)$ 为 $X$ 的边缘分布函数

**独立性**指 $P(X\leq x,Y\leq y)=P(X\leq x)P(Y\leq y)$ 或者 $F(x,y)=F_X(x)F_Y(y)$

#### 连续型

如果存在非负可积函数 $f(x,y)$ 使对任意实数对 $(x,y)$：

$\displaystyle F(x,y)=\int_{-\infty}^x\int_{-\infty}^y f(u,v)dudv$

则称 $(X,Y)$ 为二维连续型随机变量，称 $f(x,y)$ 为其联合概率密度函数

根据概率密度的定义可知概率密度函数 $f(x, y)$ 满足如下性质:

1) $\displaystyle f(x, y) \geqslant 0$.
2) $\displaystyle \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) d x d y=1$
3) 若 $f(x, y)$ 在 $(x, y)$ 连续, 则 $\displaystyle f(x, y)=\frac{\partial^{2} F(x, y)}{\partial x \partial y}$.
4) 若 $G$ 为平面上的一个区域, 则点 $(X, Y)$ 落入 $G$ 的概率为$\displaystyle P((X, Y) \in G)=\iint_{(x, y) \in G} f(x, y) d x d y$
5) 边缘概率密度函数 $\displaystyle f_X(x)=\int_{-\infty}^{+\infty} f(x,y)dy$，$y$ 同理。

对任意 $x \in(-\infty,+\infty)$ 和 $y \in(-\infty,+\infty)$, 若二维连续随机变量 $(X, Y)$ 的概率密度 与边缘概率密度满足

$f(x, y)=f_{X}(x) f_{Y}(y)$

则随机变量 $X$ 和 $Y$ 相互独立

#### ==正态分布==

设 $|\rho|<1$, 令
$$
\mu=\left(\mu_{x}, \mu_{y}\right)^{\top}=\left(\begin{array}{c}
\mu_{x} \\
\mu_{y}
\end{array}\right) \quad \text { 和 } \quad \Sigma=\left(\begin{array}{cc}
\sigma_{x}^{2} & \rho \sigma_{x} \sigma_{y} \\
\rho \sigma_{x} \sigma_{y} & \sigma_{y}^{2}
\end{array}\right)
$$
若随机变量 $X$ 和 $Y$ 的==联合概率密度函数==为
$$
\begin{aligned}
&f(x, y)=(2 \pi)^{-2 / 2}|\Sigma|^{-1 / 2} \exp \left(-\frac{1}{2}(\xi-\mu)^{\top} \Sigma^{-1}(\xi-\mu)\right)  \\
&\quad=\frac{1}{2 \pi \sqrt{1-\rho^{2}} \sigma_{x} \sigma_{y}} \exp \left(-\frac{1}{2\left(1-\rho^{2}\right)}\left[\frac{\left(x-\mu_{x}\right)^{2}}{\sigma_{x}^{2}}+\frac{\left(y-\mu_{y}\right)^{2}}{\sigma_{y}^{2}}-\frac{2 \rho}{\sigma_{x} \sigma_{y}}\left(x-\mu_{x}\right)\left(y-\mu_{y}\right)\right]\right)
\end{aligned}
$$
其中 $\xi=(x, y)^{\top}$

这里利用 $|\Sigma|=\left(1-\rho^{2}\right) \sigma_{x}^{2} \sigma_{y}^{2}$, 以及
$$
\Sigma^{-1}=\frac{1}{\left(1-\rho^{2}\right) \sigma_{x}^{2} \sigma_{y}^{2}}\left(\begin{array}{cc}
\sigma_{y}^{2} & -\rho \sigma_{x} \sigma_{y} \\
-\rho \sigma_{x} \sigma_{y} & \sigma_{x}^{2}
\end{array}\right)=\frac{1}{1-\rho^{2}}\left(\begin{array}{cc}
1 / \sigma_{x}^{2} & -\rho / \sigma_{x} \sigma_{y} \\
-\rho / \sigma_{x} \sigma_{y} & 1 / \sigma_{y}^{2}
\end{array}\right)
$$
则称随机变量 $X$ 和 $Y$ 服从参数为 $\mu$ 和 $\Sigma$ 的**正态分布**, 记为 $(X, Y) \sim \mathcal{N}(\mu, \Sigma)$

边缘分布为 $X \sim \mathcal{N}\left(\mu_{x}, \sigma_{x}^{2}\right)$ 和 $Y \sim \mathcal{N}\left(\mu_{y}, \sigma_{y}^{2}\right)$.

若 $\mu=(0,0)^{\top}$ 和 $\Sigma$ 为二维单位阵, 则称为**二维标准正态分布**

若二维随机变量 $(X, Y) \sim \mathcal{N}(\mu, \Sigma)$, 则 $X$ 与 $Y$ 独立的充要条件为 $\Sigma=\left(\begin{array}{cc}\sigma_{x}^{2} & 0 \\ 0 & \sigma_{y}^{2}\end{array}\right)$

### $Z=g(X,Y)$ 的分布

对于**离散型**随机变量 $Z=g(X_1,\cdots,X_n)$，计算不同组合下的取值，然后合并 $Z$ 相同的组合

对于**连续**随机变量 $(X, Y)$, 其联合概率密度为 $f(x, y)$, 如何求解随机变量 $Z=g(X, Y)$ 的概率 密度. 针对此类问题, 主要求解思路为分布函数法, 即

1. ==求 $Z=g(X, Y)$ 的分布函数==

$$
F_{Z}(z)=P(Z \leqslant z)=P(g(x, y) \leqslant z)=\iint_{g(x, y) \leqslant z} f(x, y) d x d y
$$

2. 求 $Z$ 的密度函数 $f_{Z}(z)=F_{Z}^{\prime}(z)$

#### 极大极小分布

设 $X_{1}, X_{2}, \cdots, X_{n}$ 为 $n$ 个相互独立的随机变量, 其分布函数分别为 $F_{X_{i}}\left(x_{i}\right)$, 则随机 变量 $Y=\max \left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 的分布函数为
$$
F_{Y}(y)=F_{X_{1}}(y) F_{X_{2}}(y) \cdots F_{X_{n}}(y)
$$
随机变量 $Z=\min \left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 的分布函数为
$$
F_{Z}(z)=1-\left(1-F_{X_{1}}(z)\right)\left(1-F_{X_{2}}(z)\right) \cdots\left(1-F_{X_{n}}(z)\right)
$$

#### ==和的分布==

随机变量 $Z=X+Y$ 的概率密度为
$$
f_{Z}(z)=\int_{-\infty}^{+\infty} f(x, z-x) d x \quad \text { 或 } \quad f_{Z}(z)=\int_{-\infty}^{+\infty} f(z-y, y) d y
$$
**==卷积公式==**：若连续随机变量 $X$ 与 $Y$ 相互独立, 其概率密度函数分别为 $f_{X}(x)$ 和 $f_{Y}(y)$, 则随机变量 $Z=X+Y$ 的密度函数为
$$
f_{Z}(z)=\int_{-\infty}^{+\infty} f_{X}(x) f_{Y}(z-x) d x=\int_{-\infty}^{+\infty} f_{X}(z-y) f_{Y}(y) d y
$$

特殊情况：

| 名称     | 原分布                                      | 新分布                                                       |
| -------- | ------------------------------------------- | ------------------------------------------------------------ |
| 二项分布 | $X\sim B(n_1,p),Y\sim B(n_2,p)$             | $Z=X+Y\sim B(n_1+n_2,p)$                                     |
| 泊松分布 | $X\sim P(\lambda_1),Y\sim P(\lambda_2)$     | $Z=X+Y\sim P(\lambda_1+\lambda_2)$                           |
| 正态分布 | $X\sim N(\mu_1,a_1^2),Y\sim N(\mu_2,a_2^2)$ | $Z=X+Y\sim N(\mu_1+\mu_2,a_1^2+a_2^2)$                       |
| 均匀分布 | $X\sim U(0,1),Y\sim U(0,1)$                 | $\displaystyle f_Z(z)=0,\quad z\leq0\; or \;z\geq2$<br/>$\displaystyle f_Z(z)=\int_0^z dx=z,\quad z\in(0,1)$<br/>$\displaystyle f_Z(z)=\int_{z-1}^1 dx=2-z,\quad z\in(1,2)$ |

#### ==乘积的分布==

$X,Y$ 的密度为 $f(x,y)$

$Z=XY$ 满足：

$\displaystyle f_Z(z)=\int_{-\infty}^{+\infty}\frac{1}{|x|}f(x,\frac {z}{x})dx=\int_{-\infty}^{+\infty}\frac{1}{|y|}f(\frac {z}{y},y)dy$

#### ==商的分布==

$X,Y$ 的密度为 $f(x,y)$

$Z=Y/X$ 满足：

$\displaystyle f_Z(z)=\int_{-\infty}^{+\infty}|x|f(x,xz)dx$

### ==多维正态分布==

设向量 $\mu \in \mathbb{R}^{n}$ 和正定矩阵 $\Sigma \in \mathbb{R}^{n \times n}$, 若随机向量 $\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 的概率密度函数为
$$
f\left(x_{1}, \ldots, x_{n}\right)=(2 \pi)^{-n / 2}|\Sigma|^{-1 / 2} \exp \left(-\frac{1}{2}(\xi-\mu)^{\top} \Sigma^{-1}(\xi-\mu)\right)
$$
其中 $\xi=\left(x_{1}, \ldots, x_{n}\right)^{\top}$, 则称随机向量 $\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 服从参数为 $\mu$ 和 $\Sigma$ 的多维正态分布, 记作
$$
\left(X_{1}, X_{2}, \cdots, X_{n}\right) \sim \mathcal{N}(\mu, \Sigma)
$$
边缘分布仍然是正态分布

独立性道理一样

**标准化**：

可以不复，考了认命

对正定矩阵 $\Sigma$, 其特征值分解为
$$
\Sigma=U^{\top} \Lambda U
$$
其中 $\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{n}\right)$ 为特征值构成的对角阵, $U$ 为特征向量构成的正交矩阵. 我们有如下多维正态分布的标准正态化:

若随机向量 $X=\left(X_{1}, X_{2}, \cdots, X_{n}\right) \sim \mathcal{N}(\mu, \Sigma)$, 且正定矩阵 $\Sigma$ 的特征值分解为 $\Sigma=U^{\top} \Lambda U$, 则随机向量
$$
Y=\Lambda^{-1 / 2} U(X-\mu) \sim \mathcal{N}\left(0_{n}, I_{n}\right)
$$
其中 $0_{n}$ 为全为零的 $n$ 维向量, $I_{n}$ 表示 $n \times n$ 的单位阵

## 多维随机变量的数字特征

### 期望

对任意随机变量 $X,Y$ 和常数 $a,b$，有 $E[aX+bY]=aE[X]+bE[Y]$

对独立的随机变量 $X,Y$ 和任意函数 $h(x),g(y)$ ，有 $E[h(X)g(Y)]=E[h(X)]E[g(Y)]$（反之不然，见独立性与相关性一节）

对任意随机变量 $X,Y$ 有Cauchy-Schwartz不等式 ==$E[XY]\leq \sqrt{E[X^2]E[Y^2]}$==

随机变量 $\boldsymbol{Z}=\boldsymbol{g}(\boldsymbol{X}, \boldsymbol{Y})$ 的期望为：

* 离散型：$\displaystyle E[Z]=E[g(X, Y)]=\sum_{i, j} g\left(x_{i}, y_{j}\right) p_{i j}$
* 连续型：$\displaystyle E[Z]=E[g(X, Y)]=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x, y) f(x, y) d x d y$

### 方差和协方差

==$Var(X+Y)=Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))]$==

当 $X,Y$ 独立时 $Var(X+Y)=Var(X)+Var(Y)$（反之不然，见独立性与相关性一节）

减法也是加号。

==$Cov(X,Y)=E[(X-E(X))(Y-E(Y))]=E[XY]-E[X]E[Y]$==

求协方差：求出三个期望（不需要求边缘分布函数，==因为 $\displaystyle E[X]=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}xf(x,y)dxdy$==）

* $Cov(X,X)=Var(X)$
* 交换律：$Cov(X,Y)=Cov(Y,X)$
* $Cov(aX,bY)=abCov(X,Y)$
* $Cov(X+a,Y+b)=Cov(X,Y)$
* $Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$
* 两个独立变量的协方差为 $0$
* $\displaystyle\operatorname{Cov}\left(\sum_{i=1}^{n} X_{i}, \sum_{j}^{m} Y_{j}\right)=\sum_{i=1}^{n} \sum_{j=1}^{m} \operatorname{Cov}\left(X_{i}, Y_{j}\right)$
* $\displaystyle \operatorname{Var}\left(\sum_{i}^{n} X_{i}\right)=\sum_{i}^{n} \operatorname{Var}\left(X_{i}\right)+2 \sum_{i<j} \operatorname{Cov}\left(X_{i}, X_{j}\right)$

## ==相关系数==

$$
\rho_{X Y}=\frac{\operatorname{Cov}(X, Y)}{\sqrt{\operatorname{Var}(X) \operatorname{Var}(Y)}}
$$

这个就是二维正态分布里面那个 $\rho$

* 相关系数 $\left|\rho_{X Y}\right| \leq 1:$ 
  * 若 $\rho>0, X$ 与 $Y$ ==正相关==;
  * 若 $\rho<0, X$ 与 $Y$ 负相关;
* $\left|\rho_{X Y}\right|=1$ 的充要条件为 $X$ 与 $Y$ 有==线性关系== $Y=a X+b$
* 若相关系数 $\left|\rho_{X Y}\right|=0$, 则随机变量 $X$ 和 $Y$ 不相关，（不代表 $X$ 和 $Y$ 独立，这里的相关只是线性相关）

==相关但不独立==的反例：

* $X\sim U(-1,1)$
* $Y=(X==0?1:0)$（这不是数学写法，明白就行）
* 此时不独立，但不相关

## 多维随机变量的期望和协方差矩阵

设 $X=\left(X_{1}, X_{2}, \cdots, X_{n}\right)^{\top}$, 则随机向量的**期望**
$$
E(X)=\left(E\left(X_{1}\right), E\left(X_{2}\right), \cdots, E\left(X_{n}\right)\right)^{\top}
$$
称随机变量 $X$ 的**协方差矩阵**为
$$
\operatorname{Cov}(X)=\Sigma=\left(\begin{array}{ccc}
\operatorname{Cov}\left(X_{1}, X_{1}\right) & \cdots & \operatorname{Cov}\left(X_{1}, X_{n}\right) \\
\operatorname{Cov}\left(X_{2}, X_{1}\right) & \cdots & \operatorname{Cov}\left(X_{2}, X_{n}\right) \\
\vdots & & \vdots \\
\operatorname{Cov}\left(X_{n}, X_{1}\right) & \cdots & \operatorname{Cov}\left(X_{n}, X_{n}\right)
\end{array}\right)
$$
==协方差矩阵是半正定矩阵，它就是多维正态分布里那个矩阵==

多个正态分布（不需要独立）的联合分布是多维正态分布

## 条件分布与条件期望

**离散型**：
$$
P\left(X=x_{i} \mid Y=y_{j}\right)=\frac{P\left(X=x_{i}, Y=y_{j}\right)}{P\left(Y=y_{j}\right)}=\frac{p_{i j}}{p_{\cdot j}} \quad(i=1,2, \cdots)
$$
**连续型**：

$Y=y$ 条件下随机变量 $X$的：

* 条件概率密度：$\displaystyle f_{X \mid Y}(x \mid y)=\frac{f(x, y)}{f_{Y}(y)}$
* 条件分布函数 $\displaystyle F_{X \mid Y}(x \mid y)=P\{X \leqslant x \mid Y=y\}=\int_{-\infty}^{x} f_{X \mid Y}(u \mid y) d u$
* 乘法公式：$f(x,y)=f_X(x)f_{Y|X}(y|x)$
* 独立性：$f_{Y|X}(y|x)=f_Y(y)$
* 期望：$\displaystyle E[X|Y=y]=\int_{-\infty}^{+\infty}xf(x|y)dx$
* 全期望公式：$E[X]=E[X|A]P(A)+E[X|\bar{A}](1-P(A))$

## ==集中不等式==

==Markov不等式: $\displaystyle P(X \geq \epsilon) \leq \frac{E(X)}{\epsilon}$==

==Chebyshev不等式: $\displaystyle P(|X-\mu|>\epsilon) \leq \frac{\operatorname{Var}(X)}{\epsilon^{2}}$==

~~单边Chebyshev不等式: $\displaystyle P(X-\mu \geq \epsilon) \leq \frac{\sigma^{2}}{\sigma^{2}+\epsilon^{2}}$~~

~~Hölder不等式: $\displaystyle E(|X Y|) \leq\left(E\left(|X|^{p}\right)\right)^{\frac{1}{p}}\left(E\left(|Y|^{q}\right)\right)^{\frac{1}{q}}$~~

随机变量 $X$ 的矩生成函数为 $M_{X}(t)=E\left[e^{t X}\right]$

* $E\left[X^{n}\right]=M_{X}^{(n)}(0)$，这里 $M_{X}^{(n)}(t)$ 表示矩生成函数在 $t=0$ 的 $n$ 阶导数, 而 $E\left[X^{n}\right]$ 被称为随机变量 $X$ 的 $n$ 阶矩 (moment)
* 对任意独立的随机变量 $X$ 和 $Y$ 有 $M_{X+Y}(t)=M_{X}(t) M_{Y}(t)$

### ==Chernoff方法==

下面将利用矩生成函数来证明一系列不等式. 给定任意随机变量 $X$ 和任意 $t>0$ 和 $\epsilon>0$, 利用 Markov 不等式有
$$
\operatorname{Pr}[X \geqslant E[X]+\epsilon]=\operatorname{Pr}\left[e^{t X} \geqslant e^{t E[X]+t \epsilon}\right] \leqslant e^{-t \epsilon-t E[X]} E\left[e^{t X}\right]
$$
特别地, 有
$$
\operatorname{Pr}[X-E[X] \geqslant \epsilon] \leqslant \min _{t>0}\left\{e^{-t \epsilon-t E[X]} E\left[e^{t X}\right]\right\}
$$

> 复习时注意讲义里面上面的公式是写错了的

类似地, 对任意 $\epsilon>0$ 和 $t<0$ 有
$$
\operatorname{Pr}[X \leqslant E[X]-\epsilon]=\operatorname{Pr}\left[e^{t X} \geqslant e^{t E[X]-t \epsilon}\right] \leqslant e^{t \epsilon-t E[X]} E\left[e^{t X}\right]
$$
同理有
$$
\operatorname{Pr}[X \leqslant \epsilon] \leqslant \min _{t<0}\left\{e^{t \epsilon-t E[X]} E\left[e^{t X}\right]\right\}
$$
上述方法称为 **Chernoff 方法**，是证明集中不等式一种最根本最重要的方法. 下面将针对特定的分布或特定的条件, 先求解矩生成函数 $E\left[e^{t X}\right]$, 然后求解最小值 $t$ 的取值

* $0 / 1$-随机变量 $\displaystyle P\left[\sum_{i=1}^{n} X_{i} \geq(1+\epsilon) \mu\right] \leq\left(\frac{e^{\epsilon}}{(1+\epsilon)^{1+\epsilon}}\right)^{\mu}$
  * 设随机变量 $X_{1}, X_{2}, \cdots, X_{n}$ 相互独立且满足 $X_{i} \sim \operatorname{Ber}\left(p_{i}\right)$, 令 $\mu=\sum_{i=1}^{n} E\left[X_{i}\right]=$ $\sum_{i=1}^{n} p_{i}$
  * $\operatorname{Pr}\left[\sum_{i=1}^{n} X_{i} \geqslant(1+\epsilon) \mu\right] \leqslant e^{-\mu \epsilon^{2} / 3}$
* Rademacher随机变量: $\displaystyle P\left[\frac{1}{n} \sum_{i=1}^{n} X_{i} \geq \epsilon\right] \leq e^{-n \epsilon^{2} / 2}$
  * $\operatorname{Pr}(X=+1)=\operatorname{Pr}(X=-1)=1 / 2$
* 有界: Chernoff引理 $\displaystyle P\left[\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} E\left[X_{i}\right] \geq \epsilon\right] \leq e^{-2 n \epsilon^{2} /(b-a)^{2}}$
* Sub-Gaussian随机变量: $\displaystyle \operatorname{Pr}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right) \geqslant \epsilon\right] \leqslant \exp \left(-n \epsilon^{2} / 2 b\right) \quad \text { 和 } \quad \operatorname{Pr}\left[\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\mu\right) \leqslant-\epsilon\right] \leqslant \exp \left(-n \epsilon^{2} / 2 b\right) \text {. }$
  * 有界随机变量、Gaussian随机变量都是亚高斯随机变量
  * 亚高斯随机变量指 $E\left[e^{(X-E[X]) t}\right] \leqslant \exp \left(b t^{2} / 2\right)$
  * 亚高斯随机变量表示随机变量的尾部分布不会比一个高斯分布更严重

求解矩生成函数：

*  $1+x \leqslant e^{x}$

* $$
  \frac{1}{2} \exp (t)+\frac{1}{2} \exp (-t)=\sum_{i \geqslant 0} \frac{t^{2 i}}{(2 i) !} \leqslant \sum_{i \geqslant 0} \frac{\left(t^{2} / 2\right)^{i}}{i !}=\exp \left(t^{2} / 2\right)
  $$

* $e^{t X}=e^{t X+(1-X) 0} \leqslant X e^{t}+(1-X) e^{0}$

* 正态分布的性质 $P\left(X^{\prime} \geqslant \epsilon\right) \leqslant \frac{1}{2} e^{-\epsilon^{2} / 2}$

## 大数定律

#### ==依概率收敛==

设 $X_{1}, X_{2}, \cdots, X_{n}, \cdots$ 是一随机变量序列, $a$ 是一常数, 如果对任意 $\epsilon>0$ 有
$$
\lim _{n \rightarrow \infty} \operatorname{Pr}\left\{\left|X_{n}-a\right|<\epsilon\right\}=1 \text { 或 } \lim _{n \rightarrow \infty} \operatorname{Pr}\left\{\left|X_{n}-a\right|>\epsilon\right\}=0
$$
则称随机变量序列 $X_{1}, X_{2}, \cdots, X_{n}, \cdots$ 依概率收敛于 $a$, 记 $X_{n} \stackrel{P}{\rightarrow} a$

与数列收敛不一样的是：依概率收敛的数列是不确定的

下面我们给出依概率收敛的性质:

1) 若 $X_{n} \stackrel{P}{\rightarrow} a$ 且函数 $g: \mathbb{R} \rightarrow \mathbb{R}$ 在 $X=a$ 点连续, 则 $g\left(X_{n}\right) \stackrel{P}{\rightarrow} g(a)$.
2) 若 $X_{n} \stackrel{P}{\rightarrow} a, Y_{n} \stackrel{P}{\rightarrow} b$, 函数 $g: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ 在点 $(X, Y)=(a, b)$ 处连续, 则 $g\left(X_{n}, Y_{n}\right) \stackrel{P}{\rightarrow} g(a, b)$.
   例如: 如果 $X_{n} \stackrel{P}{\rightarrow} a$ 和 $Y_{n} \stackrel{P}{\rightarrow} b$, 那么 $X_{n}+Y_{n} \stackrel{P}{\rightarrow} a+b$ 和 $X_{n} Y_{n} \stackrel{P}{\rightarrow} a b$ 

### 大数定律

若随机变量序列 $X_{1}, X_{2}, \cdots, X_{n}, \cdots$ 满足
$$
\frac{1}{n} \sum_{i=1}^{n} X_{i} \stackrel{P}{\rightarrow} \frac{1}{n} \sum_{i=1}^{n} E\left[X_{i}\right]
$$
则称 $\left\{X_{n}\right\}$ 服从大数定律

大数定理刻画了随机变量的均值 (算术平均值) 依概率收敛于期望的均值 (算术平均值)

| 名称               | 基本条件                  | 条件                                                         |
| ------------------ | ------------------------- | ------------------------------------------------------------ |
| 马尔可夫 Markov    | 无                        | $\displaystyle \frac{1}{n^{2}} \operatorname{Var}\left(\sum_{i=1}^{n} X_{i}\right) \rightarrow 0 \quad n \rightarrow \infty$ |
| 切比雪夫 Chebyshev | 相互独立                  | 存在常数 $c>0$ 使得 $\operatorname{Var}\left(X_{i}\right) \leqslant c$, |
| 辛钦 Khintchine    | 独立同分布                | 每个随机变量的期望 $E\left[X_{i}\right]=\mu$ 存在            |
| 伯努利 Bernoulli   | $X_{n} \sim B(n, p)(p>0)$ |                                                              |

伯努利导出的是 $\displaystyle \lim _{n \rightarrow \infty} \operatorname{Pr}\left[\left|\frac{X_{n}}{n}-p\right| \geqslant \epsilon\right]=0$

==如何判断随机变量序列 $X_{1}, X_{2}, \cdots, X_{n}, \cdots$ 满足大数定律==:

- 是否是二值随机变量
- 若随机变量独立同分布, 则利用辛钦大数定律查看期望是否存在;
- 独立但不同分布，切比雪夫检查方差上界
- 对非独立同分布随机变量, 则利用 Markov 大数定律判断方差是否趋于零.

## ==中心极限定理==

### 依分布收敛

定义 $7.2$ 设随机变量 $Y$ 的分布函数为 $F_{Y}(y)=\operatorname{Pr}(Y \leqslant y)$, 以及随机变量序列 $Y_{1}, Y_{2}, \cdots, Y_{n}, \cdots$ 的分布函数分别为 $F_{Y_{n}}(y)=\operatorname{Pr}\left(Y_{n} \leqslant y\right)$, 如果
$$
\lim _{n \rightarrow \infty} \operatorname{Pr}\left[Y_{n} \leqslant y\right]=\operatorname{Pr}[Y \leqslant y] \text {, 即 } \lim _{n \rightarrow \infty} F_{Y_{n}}(y)=F_{Y}(y) \text {, }
$$
则称随机变量序列 $Y_{1}, Y_{2}, \cdots, Y_{n}, \cdots$ 依分布收敛于 $Y$, 记 $Y_{n} \stackrel{d}{\rightarrow} Y$

* 林德贝格-勒维中心极限定理：**独立同分布**随机变量, 若 $E\left[X_{k}\right]=\mu$ 和 $\operatorname{Var}\left(X_{k}\right)=\sigma^{2}$, 则 $\sum_{k=1}^{n} X_{k} \stackrel{d}{\rightarrow} N\left(n \mu, n \sigma^{2}\right)$
  * $\displaystyle Y_n=\frac{\sum_{i=1}^n X_i-n\mu}{\sigma\sqrt{n}}\stackrel{d}{\rightarrow}N(0,1)$

* 棣莫弗-拉普拉斯中心极限定理：若 $X_{n} \sim B(n, p)$, $X_{n} \stackrel{d}{\rightarrow} N(n p, n p(1-p))$
  * $\displaystyle Y_{n}=\frac{X_{n}-n p}{\sqrt{n p(1-p)}} \stackrel{d}{\rightarrow} \mathcal{N}(0,1)$


* 李雅普诺夫定理: **独立不同分布**中心极限定理 $\displaystyle \frac{1}{B_{n}^{2+\delta}} \sum_{k=1}^{n} E\left[\left|X_{k}-\mu_{k}\right|^{2+\delta}\right] \rightarrow 0$

## 统计基本概念

**总体**: 研究对象的全体, 用随机变量 $X$ 表示(分布未知) 

**样本**: 从总体中随机抽取一些个体, 表示为 $X_{1}, X_{2}, \cdots, X_{n}$, 称 $X_{1}, X_{2}, \cdots, X_{n}$ 为取自总体 $X$ 的随机样本, 其样本容量为 $n$ 

称样本 $X_{1}, X_{2}, \cdots, X_{n}$ 是总体 $X$ 的==**简单随机样本**==, 简称样本, 是指样本满足: 

1. 代表性, 即 $X_{i}$ 与 $X$ 同分布;
2. 独立性, 即 $X_{1}, X_{2}, \cdots, X_{n}$ 之间相互独立.

**抽样**: 抽取样本的过程

**样本值**: 观察样本得到的数值, 例如: $X_{1}=x_{1}, X_{2}=x_{2}, \cdots, X_{n}=x_{n}$ 为样本观察值或样本值

**样本的==二重性==**：

* 就一次具体观察而言, 样本值是确定的数
* 不同的抽样下, 样本值会发生变化, 可看作随机变量

$\begin{aligned} \text { 样本的分布: } & F\left(x_{1}, x_{2}, \cdots, x_{n}\right)=F\left(x_{1}\right) F\left(x_{2}\right) \cdots F\left(x_{n}\right) \\ & f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=f\left(x_{1}\right) f\left(x_{2}\right) \cdots f\left(x_{n}\right) \\ & P\left(X_{1}=x_{1}, X_{2}=x_{2}, \cdots, X_{n}=x_{n}\right)=\prod_{i=1}^{n} P\left(X_{i}=x_{i}\right) \end{aligned}$

统计量: $g\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 关于 $X_{1}, X_{2}, \cdots, X_{n}$ 连续、不含任意参数

| 名称          | 符号      | 定义                                                         | 与总体关系                                                   |
| ------------- | --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 均值          | $\bar{X}$ | $\displaystyle \bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$     | $\bar{X} \stackrel{d}{\rightarrow} \mathcal{N}\left(\mu, \sigma^{2} / n\right)$ |
| 方差          | $S_0^2$   | $\displaystyle S_{0}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}-\bar{X}^{2} .$ | $\displaystyle E\left[S_{0}^{2}\right]=\frac{n-1}{n} \sigma^{2}$ |
| 标准差        | $S_0$     |                                                              |                                                              |
| 修正方差      | $S^2$     | $\displaystyle S^{2}=\frac{n}{n-1} S_{0}^{2}$                | $E\left[S^{2}\right]=\sigma^{2}$                             |
| $k$ 阶 原点矩 | $A_k$     | $\displaystyle A_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}, \quad k=1,2, \cdots$ | 样本与零的k阶平均距离                                        |
| $k$ 阶 中心矩 | $B_k$     | $\displaystyle B_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k}, \quad k=1,2, \cdots$ | 样本与期望的k阶平均距离                                      |

假设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $X$ 的一个样本, 定义最小次序统计量和 最大次序统计量 分 别为:
$$
X_{(1)}=\min \left\{X_{1}, X_{2}, \cdots, X_{n}\right\} \quad \text { 和 } \quad X_{(n)}=\max \left\{X_{1}, X_{2}, \cdots, X_{n}\right\} \text {, }
$$
以及定义样本极差为
$$
R_{n}=X_{(n)}-X_{(1)}
$$
设总体 $X$ 的分布函数为 $F(x)$, 则有
$$
F_{X_{(1)}}(x)=\operatorname{Pr}\left(X_{(1)} \leqslant x\right)=1-\operatorname{Pr}\left(X_{(1)}>x\right)=1-(1-F(x))^{n}, \quad F_{X_{(n)}}(x)=F^{n}(x) .
$$

## 三大统计分布

### $\chi^{2}$ 分布

若 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $X \sim \mathcal{N}(0,1)$ 的一个样本

$Y=X_{1}^{2}+X_{2}^{2}+\cdots+X_{n}^{2}\sim \chi^2(n)$ ，

$E(Y)=n$ ， $\operatorname{Var}(Y)=2 n$

标准正态分布的平方是 $\chi^2(1)$

若随机变量 $X \sim \mathcal{N}(0,1)$, 则
$$
E\left(X^{k}\right)= \begin{cases}(k-1) ! ! & k \text { 为偶数 } \\ 0 & k \text { 为奇数 }\end{cases}
$$
其中 $(2 k) ! !=2 k \cdot(2 k-2) \cdots 2$ 和 $(2 k+1) ! !=(2 k+1) \cdot(2 k-1) \cdots 1$

#### 可加性

若随机变量 $X \sim \chi^{2}(m)$ 和 $Y \sim \chi^{2}(n)$ 相互独立, 则 $X+Y \sim \chi^{2}(m+n)$

回顾分布可加性:

- 如果 $X \sim \mathcal{N}\left(\mu_{1}, a_{1}^{2}\right)$ 和 $Y \sim \mathcal{N}\left(\mu_{2}, a_{2}^{2}\right)$, 且. $X$ 与 $Y$ 独立, 那么 $X \pm Y \sim \mathcal{N}\left(\mu_{1} \pm \mu_{2}, a_{1}^{2}+a_{2}^{2}\right)$;
- 如果 $X \sim B\left(n_{1}, p\right)$ 和 $Y \sim B\left(n_{2}, p\right)$, 且 $X$ 与 $Y$ 独立, 那么 $X+Y \sim B\left(n_{1}+n_{2}, p\right)$;
- 如果 $X \sim P\left(\lambda_{1}\right)$ 和 $Y \sim P\left(\lambda_{2}\right)$, 且 $X$ 与 $Y$ 独立, 那么 $X+Y \sim P\left(\lambda_{1}+\lambda_{1}\right)$;
- 如果 $X \sim \Gamma\left(\alpha_{1}, \lambda\right)$ 和 $Y \sim \Gamma\left(\alpha_{2}, \lambda\right)$, 且. $X$ 与 $Y$ 独立, 那么 $X+Y \sim \Gamma\left(\alpha_{1}+\alpha_{2}, \lambda\right)$.
- 如果 $X \sim \chi^2(m)$ 和 $Y \sim \chi^2(n)$, 且 $X$ 与 $Y$ 独立, 那么 $X+Y \sim \chi^2(m+n)$.

### $t$ 分布

随机变量 $X \sim \mathcal{N}(0,1)$ 和 $Y \sim \chi^{2}(n)$ 相互独立

则 ==$\displaystyle T=\frac{X}{\sqrt{Y / n}}$== 服从自由度为 $n$ 的 $t$-分布, 记 $T \sim t(n)$.

随机变量 $T \sim t(n)$ 的概率密度为
$$
f(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right) \sqrt{n \pi}}\left(1+\frac{x^{2}}{n}\right)^{-\frac{n+1}{2}} \quad x \in(-\infty,+\infty) .
$$

由此可知 $t$-分布的密度函数 $f(x)$ 是偶函数.

当 $n>1$ 为偶数时有
$$
\frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right) \sqrt{n \pi}}=\frac{(n-1)(n-3) \cdots 5 \cdot 3}{2 \sqrt{n}(n-2)(n-4) \cdots 4 \cdot 2}
$$
当 $n>1$ 为奇数时有
$$
\frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right) \sqrt{n \pi}}=\frac{(n-1)(n-3) \cdots 4 \cdot 2}{\pi \sqrt{n}(n-2)(n-4) \cdots 5 \cdot 3} .
$$
当 $n \rightarrow \infty$ 时, 随机变量 $T \sim t(n)$ 的概率密度
$$
f(x) \rightarrow \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^{2}}{2}}
$$
因此当 $n$ 足够大时, $f(x)$ 可被近似为 $\mathcal{N}(0,1)$ 的密度函数

### F 分布

定义 $8.11$ 设随机变量 $X \sim \chi^{2}(m)$ 和 $Y \sim \chi^{2}(n)$ 相互独立, 称==随机变量==
$$
F=\frac{X / m}{Y / n}
$$

服从自由度为 $(m, n)$ 的 $F$-分布, 记 $F \sim F(m, n)$.

随机变量 $F \sim F(m, n)$ 的概率密度为
$$
f(x)= \begin{cases}\frac{\Gamma\left(\frac{m+n}{2}\right)\left(\frac{m}{n}\right)^{\frac{m}{2}} x^{\frac{m}{2}-1}}{\Gamma\left(\frac{m}{2}\right) \Gamma\left(\frac{n}{2}\right)\left(1+\frac{m x}{n}\right)^{\frac{m+n}{2}}} & x>0 \\ 0 & x \leqslant 0\end{cases}
$$
若随机变量 $F \sim F(m, n)$, 则 $\frac{1}{F}=F(n, m)$

## 五大抽样分布定理

### 抽样分布定理一

定理 $8.8$ 设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $\mathcal{N}\left(\mu, \sigma^{2}\right)$ 的样本, 则有
$$
\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i} \sim \mathcal{N}\left(\mu, \frac{\sigma^{2}}{n}\right), \quad \frac{\bar{X}-\mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1)
$$

### 抽样分布定理二

定理 $8.9$ 设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $\mathcal{N}\left(\mu, \sigma^{2}\right)$ 的样本, 其样本均值和修正样本方差分别为
$$
\bar{X}=\sum_{i=1}^{n} X_{i} / n \quad \text { 和 } \quad S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} \text {, }
$$
==则有 $\bar{X}$ 和 $S^{2}$ 相互独立, 且.==
$$
\frac{S^{2}}{\sigma^{2}} \sim \frac{\chi^{2}(n-1)}{n-1}
$$
此定理证明参考书的附件

### ==抽样分布定理三==

定理 $8.10$ 设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $\mathcal{N}\left(\mu, \sigma^{2}\right)$ 的样本, 其样本均值和修正样本方差分别 为
$$
\bar{X}=\sum_{i=1}^{n} X_{i} / n \quad \text { 和 } \quad S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} \text {, }
$$
则有
$$
\frac{\bar{X}-\mu}{S / \sqrt{n}} \sim t(n-1)
$$

### ==抽样分布定理四==

定理 $8.11$ 设 $X_{1}, X_{2}, \cdots, X_{m}$ 和 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 分别来自总体 $\mathcal{N}\left(\mu_{X}, \sigma^{2}\right)$ 和 $\mathcal{N}\left(\mu_{Y}, \sigma^{2}\right)$ 的两个 独立样本, 令其样本均值分别 $\bar{X}$ 和 $\bar{Y}$, 修正样本方差分别为 $S_{X}^{2}$ 和 $S_{Y}^{2}$, 则
$$
\frac{\bar{X}-\bar{Y}-\left(\mu_{X}-\mu_{Y}\right)}{\sqrt{\frac{(m-1) S_{X}^{2}+(n-1) S_{Y}^{2}}{m+n-2} \sqrt{\frac{1}{m}+\frac{1}{n}}}} \sim t(m+n-2)
$$

### ==抽样分布定理五==

定理 $8.12$ 设 $X_{1}, X_{2}, \cdots, X_{m}$ 和 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 分别来自总体 $\mathcal{N}\left(\mu_{X}, \sigma_{X}^{2}\right)$ 和 $\mathcal{N}\left(\mu_{Y}, \sigma_{Y}^{2}\right)$ 的两 个独立样本, 令其修正样本方差分别为 $S_{X}^{2}$ 和 $S_{Y}^{2}$, 则有
$$
\frac{S_{X}^{2} / \sigma_{X}^{2}}{S_{Y}^{2} / \sigma_{Y}^{2}} \sim F(m-1, n-1)
$$

## 参数估计

### 矩估计

矩估计方法: 总体 $X$ 的分布函数 $F$ 包含 $m$ 个末知参数 $\theta_{1}, \theta_{2}, \cdots, \theta_{m}$,

1) 求总体 $X$ 的 $k$ 阶矩: $a_{k}=a_{k}\left(\theta_{1}, \theta_{2}, \cdots, \theta_{m}\right)=E\left[X^{k}\right], k \in[m]\left(a_{k}\right.$ 一般为 $\theta_{1}, \theta_{2}, \cdots, \theta_{m}$ 的 函数).
   * 注意不一定是 $X^k$，比如 $X^2$ 可以用方差代替。我们只是想要 $m$ 条独立方程而已
2) 计算样本的 $k$ 阶矩: $A_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}$.
3) 令样本矩等于总体矩 $A_{k}=a_{k}=a_{k}\left(\theta_{1}, \theta_{2}, \cdots, \theta_{m}\right)(k=1,2, \cdots, m)$, 得到 $m$ 个关于 $\theta_{1}, \theta_{2}, \cdots, \theta_{m}$ 的方程组.
4) 求解方程组得到估计量 $\widehat{\theta}_{1}, \widehat{\theta}_{2}, \cdots, \widehat{\theta}_{m}$

> * **矩估计量是参数关于X（随机变量的总体）的函数**，例如 $\displaystyle \hat{\theta}=\frac{\overline{X}}{\overline{X}-c}$
> * **矩估计值是参数关于样本值的函数**，例如 $\displaystyle \hat{\theta}=\frac{\overline{x}}{\overline{x}-c}$

### 最大似然估计

设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $X$ 的一个样本。若总体 $X$ 为离散型随机变量, 其分布列为 $\operatorname{Pr}(X=x)=\operatorname{Pr}(X=x ; \theta)$, 则样本 $X_{1}, X_{2}, \cdots, X_{n}$ 的分布列为
$$
L(\theta)=L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)=\prod_{i=1}^{n} \operatorname{Pr}\left(x_{i} ; \theta\right)
$$
这里 $L(\theta)$ 表示样本 $X_{1}=x_{1}, X_{2}=x_{2}, \cdots, X_{n}=x_{n}$ 发生的概率.

若总体 $X$ 为连续型随机变量, 其概率密度为 $f(x ; \theta)$, 则 $X_{1}=x_{1}, X_{2}=x_{2}, \cdots, X_{n}=x_{n}$ 的联合概率密度为
$$
L(\theta)=L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right)
$$
根据概率密度定义可知 $L(\theta)$ 越大, 样本 $\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 落入 $\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ 的邻域内概率越大. 综合上述离散和连续两种随机变量, 统称 $L(\theta)$ 为样本 $X_{1}, X_{2}, \cdots, X_{n}$ 的似然函数, 可以发现 $L(\theta)$ 是 $\theta$ 的函数, 若
$$
\hat{\theta}=\underset{\theta}{\arg \max } L\left(x_{1}, x_{2}, \cdots, x_{m} ; \theta\right),
$$
则称 $\hat{\theta}$ 为 $\theta$ 的最大似然估计量. 直觉而言：**最大似然估计量 $\hat{\theta}$ 是使观测值 $X_{1}=x_{1}, X_{2}=$ $x_{2}, \cdots, X_{n}=x_{n}$ 出现的概率最大.**

**求解最大似然估计量的步骤如下:**

1. 计算对数似然函数 $\ln \left(L\left(x_{1}, x_{2}, \cdots, x_{m} ; \theta\right)\right)$;
2. 求对数似然函数中参数 $\theta$ 的一阶偏导, 令其等于零;
3. 求解方程组得到最大似然估计量 $\hat{\theta}$.

### ==估计量的常用标准==

* 无偏性：无系统偏差
* 有效性：估计量的方差越小越好，有效统计量
* 一致性：在数据足够多的情况下能有效估计统计量

### 区间估计

设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自总体 $X$ 的样本, $\theta$ 为总体 $X$ 的分布函数 $F(x, \theta)$ 的未知参数, **根据样本估计 $\theta$ 的范围** $\left(\hat{\theta}_{1}, \hat{\theta}_{2}\right)$, 其中 $\hat{\theta}_{1}=\hat{\theta}_{1}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 和 $\hat{\theta}_{2}=\hat{\theta}_{2}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$, 使得以较大的概率保证有 $\theta \in\left(\hat{\theta}_{1}, \hat{\theta}_{2}\right)$ 成立. 具体而言, 对任意给定 $\alpha \in(0,1)$, 有
$$
\operatorname{Pr}\left[\hat{\theta}_{1}\left(X_{1}, X_{2}, \cdots, X_{n}\right)<\theta<\hat{\theta}_{2}\left(X_{1}, X_{2}, \cdots, X_{n}\right)\right] \geqslant 1-\alpha .
$$

#### 置信区间与置信度

设 $X_{1}, X_{2} \cdots, X_{n}$ 是来自总体 $X$ 的样本, 总体 $X$ 的分布函数 含末知参数 $\theta$, 找出统计量 $\hat{\theta}_{1}=\hat{\theta}_{1}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 和 $\hat{\theta}_{2}=\hat{\theta}_{2}\left(X_{1}, X_{2}, \cdots, X_{n}\right)\left(\hat{\theta}_{1}<\hat{\theta}_{2}\right)$, 使得
$$
\operatorname{Pr}\left[\hat{\theta}_{1}<\theta<\hat{\theta}_{2}\right] \geqslant 1-\alpha
$$
成立, 则称 $1-\alpha$ 为置信度, $\left[\hat{\theta}_{1}, \hat{\theta}_{2}\right]$ 为 $\theta$ 的置信度为 $1-\alpha$ 的置信区间.
注意: 置信区间 $\left[\hat{\theta}_{1}, \hat{\theta}_{2}\right]$ 是随机区间, $1-\alpha$ 为该区间包含 $\theta$ 的概率/可靠程度. 若 $\alpha=0.05$, 则置 信度为 $95 \%$. 通常采用 $95 \%$ 的置信度, 有时也可 $99 \%$ 或 $90 \%$ 等. 说明:

1. $\hat{\theta}_{2}-\hat{\theta}_{1}$ 反映了估计精度, 长度越小精度越大.
2. $\alpha$ 反映了估计的可靠度, $\alpha$ 越小可靠度越高.
3. 给定 $\alpha$, 区间 $\left[\hat{\theta}_{1}, \hat{\theta}_{2}\right]$ 的选取并不唯一确定, 通常选长度最小的一个区间

#### 置信区间的求解方法: 枢轴变量法

1. 先找一样本函数 $W\left(X_{1}, X_{2}, \cdots, X_{n} ; \theta\right)$ 包含待估参数 $\theta$, 但不含其它参数, 函数 $W$ 的分布已知, 称 $W$ 为**枢轴变量**.
2. 给定置信度 $1-\alpha$, 根据 $W$ 的分布找出临界值 $a$ 和 $b$, 使得 $\operatorname{Pr}[a<W<b]=1-\alpha$ 成立.
3. 根据 $a<W<b$ 解出 $\hat{\theta}_{1}<\theta<\hat{\theta}_{2}$, 则 $\left(\hat{\theta}_{1}, \hat{\theta}_{2}\right)$ 为 $\theta$ 的置信度为 $1-\alpha$ 的置信区间

### 单侧置信区间

定义 $9.5$ (单侧置信区间) 给定 $\alpha \in(0,1)$, 若 样本 $X_{1}, \cdots, X_{n}$ 的 统 计量 $\hat{\theta}_{1}=$ $\hat{\theta}_{1}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 满足
$$
\operatorname{Pr}\left[\theta>\hat{\theta}_{1}\right] \geqslant 1-\alpha,
$$
则称 $\left(\hat{\theta}_{1},+\infty\right)$ 为 $\theta$ 的置信度为 $1-\alpha$ 的单侧置信区间, $\hat{\theta}_{1}$ 称为单侧置信下限.

同理定义单侧置信上限. 对正态总体, 可以将相关置信区间的估计都扩展到单侧置信估计, 枢轴变量 的定理类似, 我们将不再重复讨论

## 假设检验

根据样本信息来检验关于总体的某个假设是否正确, 此类问题称为 假设检验问题, 可分为两类:
- 参数检验问题: 总体分布已知, 检验某末知参数的假设;
- 非参数检验问题: 总体分布末知时的假设检验问题.

假设检验的方法: 先假设所做的假设 $H_{0}$ 成立, 然后从总体中取样, 根据样本的取值来判断是否 有·不合理'的现象出现, 最后做出接受或者拒绝所做假设的决定. 不合理'的现象指小概率事件在一 次事件中几乎不会发生.
